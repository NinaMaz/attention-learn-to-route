{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec01b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from run import run\n",
    "from options import get_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8166aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = \"--graph_size 20 --baseline rollout --run_name 'tsp20_test' --no_tensorboard\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b4fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = get_options(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dddbd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(problem='tsp', graph_size=20, batch_size=512, epoch_size=1280000, val_size=10000, val_dataset=None, model='attention', embedding_dim=128, hidden_dim=128, n_encode_layers=3, tanh_clipping=10.0, normalization='batch', lr_model=0.0001, lr_critic=0.0001, lr_decay=1.0, eval_only=False, n_epochs=100, seed=1234, max_grad_norm=1.0, no_cuda=False, exp_beta=0.8, baseline='rollout', bl_alpha=0.05, bl_warmup_epochs=1, eval_batch_size=1024, checkpoint_encoder=False, shrink_size=None, data_distribution=None, log_step=50, log_dir='logs', run_name=\"'tsp20_test'_20221109T202221\", output_dir='outputs', epoch_start=0, checkpoint_epochs=1, load_path=None, resume=None, no_tensorboard=True, no_progress_bar=False, use_cuda=False, save_dir=\"outputs/tsp_20/'tsp20_test'_20221109T202221\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b124bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseline': 'rollout',\n",
      " 'batch_size': 512,\n",
      " 'bl_alpha': 0.05,\n",
      " 'bl_warmup_epochs': 1,\n",
      " 'checkpoint_encoder': False,\n",
      " 'checkpoint_epochs': 1,\n",
      " 'data_distribution': None,\n",
      " 'embedding_dim': 128,\n",
      " 'epoch_size': 1280000,\n",
      " 'epoch_start': 0,\n",
      " 'eval_batch_size': 1024,\n",
      " 'eval_only': False,\n",
      " 'exp_beta': 0.8,\n",
      " 'graph_size': 20,\n",
      " 'hidden_dim': 128,\n",
      " 'load_path': None,\n",
      " 'log_dir': 'logs',\n",
      " 'log_step': 50,\n",
      " 'lr_critic': 0.0001,\n",
      " 'lr_decay': 1.0,\n",
      " 'lr_model': 0.0001,\n",
      " 'max_grad_norm': 1.0,\n",
      " 'model': 'attention',\n",
      " 'n_encode_layers': 3,\n",
      " 'n_epochs': 100,\n",
      " 'no_cuda': False,\n",
      " 'no_progress_bar': False,\n",
      " 'no_tensorboard': True,\n",
      " 'normalization': 'batch',\n",
      " 'output_dir': 'outputs',\n",
      " 'problem': 'tsp',\n",
      " 'resume': None,\n",
      " 'run_name': \"'tsp20_test'_20221109T202221\",\n",
      " 'save_dir': \"outputs/tsp_20/'tsp20_test'_20221109T202221\",\n",
      " 'seed': 1234,\n",
      " 'shrink_size': None,\n",
      " 'tanh_clipping': 10.0,\n",
      " 'use_cuda': False,\n",
      " 'val_dataset': None,\n",
      " 'val_size': 10000}\n",
      "Evaluating baseline model on evaluation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train epoch 0, lr=0.0001 for run 'tsp20_test'_20221109T202221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                          | 1/2500 [01:27<60:31:21, 87.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_batch_id: 0, avg_cost: 10.0413818359375\n",
      "grad_norm: 27.162599563598633, clipped: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                                           | 18/2500 [01:34<19:41,  2.10it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10dc43d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1474, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/aliceperse/miniconda3/envs/attention/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  1%|▊                                                                                                         | 18/2500 [01:35<3:39:42,  5.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/run.py:185\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(opts\u001b[38;5;241m.\u001b[39mepoch_start, opts\u001b[38;5;241m.\u001b[39mepoch_start \u001b[38;5;241m+\u001b[39m opts\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[0;32m--> 185\u001b[0m         \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/train.py:121\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, baseline, lr_scheduler, epoch, val_dataset, problem, tb_logger, opts)\u001b[0m\n\u001b[1;32m    115\u001b[0m set_decode_type(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    118\u001b[0m     tqdm(training_dataloader, disable\u001b[38;5;241m=\u001b[39mopts\u001b[38;5;241m.\u001b[39mno_progress_bar)\n\u001b[1;32m    119\u001b[0m ):\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    127\u001b[0m epoch_duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/train.py:168\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(model, optimizer, baseline, epoch, batch_id, step, batch, tb_logger, opts)\u001b[0m\n\u001b[1;32m    165\u001b[0m bl_val \u001b[38;5;241m=\u001b[39m move_to(bl_val, opts\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m bl_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Evaluate model, get costs and log probabilities\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m cost, log_likelihood, reconstr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Evaluate baseline, get baseline loss if any (only for critic)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m bl_val, bl_loss \u001b[38;5;241m=\u001b[39m baseline\u001b[38;5;241m.\u001b[39meval(x, cost) \u001b[38;5;28;01mif\u001b[39;00m bl_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (bl_val, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/attention/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/nets/attention_model.py:182\u001b[0m, in \u001b[0;36mAttentionModel.forward\u001b[0;34m(self, input, return_pi)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_embed(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m--> 182\u001b[0m _log_p, pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m input_reconstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_out(embeddings\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\n\u001b[1;32m    186\u001b[0m cost, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mget_costs(\u001b[38;5;28minput\u001b[39m, pi)\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/nets/attention_model.py:344\u001b[0m, in \u001b[0;36mAttentionModel._inner\u001b[0;34m(self, input, embeddings)\u001b[0m\n\u001b[1;32m    341\u001b[0m         state \u001b[38;5;241m=\u001b[39m state[unfinished]\n\u001b[1;32m    342\u001b[0m         fixed \u001b[38;5;241m=\u001b[39m fixed[unfinished]\n\u001b[0;32m--> 344\u001b[0m log_p, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_log_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Select the indices of the next nodes in the sequences, result\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m#  (batch_size) long\u001b[39;00m\n\u001b[1;32m    348\u001b[0m selected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_node(\n\u001b[1;32m    349\u001b[0m     log_p\u001b[38;5;241m.\u001b[39mexp()[:, \u001b[38;5;241m0\u001b[39m, :], mask[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    350\u001b[0m )  \u001b[38;5;66;03m# Squeeze out steps dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/nets/attention_model.py:475\u001b[0m, in \u001b[0;36mAttentionModel._get_log_p\u001b[0;34m(self, fixed, state, normalize)\u001b[0m\n\u001b[1;32m    472\u001b[0m mask \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget_mask()\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Compute logits (unnormalized log_p)\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m log_p, glimpse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_to_many_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglimpse_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglimpse_V\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    480\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(log_p \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/attn/attention-learn-to-route/nets/attention_model.py:615\u001b[0m, in \u001b[0;36mAttentionModel._one_to_many_logits\u001b[0;34m(self, query, glimpse_K, glimpse_V, logit_K, mask)\u001b[0m\n\u001b[1;32m    609\u001b[0m glimpse_Q \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    610\u001b[0m     batch_size, num_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;241m1\u001b[39m, key_size\n\u001b[1;32m    611\u001b[0m )\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# Batch matrix multiplication to compute compatibilities\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m#    (n_heads, batch_size, num_steps, graph_size)\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m compatibility \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglimpse_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglimpse_K\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(glimpse_Q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_inner:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot mask inner without masking logits\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f520c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3792b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
